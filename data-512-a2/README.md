# data-512-a2
This folder contains a tutorial for how to analyze hostile speech data from Wikipedia Talk data to understand potential biases that would propagate through downstream models and applications trained with this data. 
## Goal 
Perform analyses on Wikipedia Talk data to identify potential sources of bias in a corpus of human-annotated data, and describe implications of those biases.

## Input
The source data is too big to be shared in the repo, however, it can be accessed directly from the source at the figsahre links below. 
- Wulczyn, Ellery; Thain, Nithum; Dixon, Lucas (2017): Wikipedia Talk Labels: Personal Attacks. figshare. Dataset. https://doi.org/10.6084/m9.figshare.4054689.v6
- Wulczyn, Ellery; Thain, Nithum; Dixon, Lucas (2017): Wikipedia Talk Labels: Aggression. figshare. Dataset. https://doi.org/10.6084/m9.figshare.4267550.v5
- Thain, Nithum; Dixon, Lucas; Wulczyn, Ellery (2017): Wikipedia Talk Labels: Toxicity. figshare. Dataset. https://doi.org/10.6084/m9.figshare.4563973.v2
- Wulczyn, Ellery; Thain, Nithum; Dixon, Lucas (2017): Wikipedia Talk Corpus. figshare. Dataset. https://doi.org/10.6084/m9.figshare.4264973.v3
    
## Output

  
## Known Facts/Issues
  
## Sources
Google data scientist use this data to train models for use in a host of applications. 
- [Perspectives API](https://github.com/conversationai/perspectiveapi/wiki/perspective-hacks)
- [Conversation AI](https://conversationai.github.io/)
